{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_unicode(text):\n",
    "    # Bảng ánh xạ các ký tự có dấu sang Unicode\n",
    "    vietnamese_chars = {\n",
    "        # Chữ a\n",
    "        'à': '\\u00E0', 'á': '\\u00E1', 'ả': '\\u1EA3', 'ã': '\\u00E3', 'ạ': '\\u1EA1',\n",
    "        'ă': '\\u0103', 'ằ': '\\u1EB1', 'ắ': '\\u1EAF', 'ẳ': '\\u1EB3', 'ẵ': '\\u1EB5', 'ặ': '\\u1EB7',\n",
    "        'â': '\\u00E2', 'ầ': '\\u1EA7', 'ấ': '\\u1EA5', 'ẩ': '\\u1EA9', 'ẫ': '\\u1EAB', 'ậ': '\\u1EAD',\n",
    "        # Chữ e\n",
    "        'è': '\\u00E8', 'é': '\\u00E9', 'ẻ': '\\u1EBB', 'ẽ': '\\u1EBD', 'ẹ': '\\u1EB9',\n",
    "        'ê': '\\u00EA', 'ề': '\\u1EC1', 'ế': '\\u1EBF', 'ể': '\\u1EC3', 'ễ': '\\u1EC5', 'ệ': '\\u1EC7',\n",
    "        # Chữ i\n",
    "        'ì': '\\u00EC', 'í': '\\u00ED', 'ỉ': '\\u1EC9', 'ĩ': '\\u0129', 'ị': '\\u1ECB',\n",
    "        # Chữ o\n",
    "        'ò': '\\u00F2', 'ó': '\\u00F3', 'ỏ': '\\u1ECF', 'õ': '\\u00F5', 'ọ': '\\u1ECD',\n",
    "        'ô': '\\u00F4', 'ồ': '\\u1ED3', 'ố': '\\u1ED1', 'ổ': '\\u1ED5', 'ỗ': '\\u1ED7', 'ộ': '\\u1ED9',\n",
    "        'ơ': '\\u01A1', 'ờ': '\\u1EDD', 'ớ': '\\u1EDB', 'ở': '\\u1EDF', 'ỡ': '\\u1EE1', 'ợ': '\\u1EE3',\n",
    "        # Chữ u\n",
    "        'ù': '\\u00F9', 'ú': '\\u00FA', 'ủ': '\\u1EE7', 'ũ': '\\u0169', 'ụ': '\\u1EE5',\n",
    "        'ư': '\\u01B0', 'ừ': '\\u1EEB', 'ứ': '\\u1EE9', 'ử': '\\u1EED', 'ữ': '\\u1EEF', 'ự': '\\u1EF1',\n",
    "        # Chữ y\n",
    "        'ỳ': '\\u1EF3', 'ý': '\\u00FD', 'ỷ': '\\u1EF7', 'ỹ': '\\u1EF9', 'ỵ': '\\u1EF5',\n",
    "        # Chữ d\n",
    "        'đ': '\\u0111',\n",
    "        # Chữ in hoa\n",
    "        'À': '\\u00C0', 'Á': '\\u00C1', 'Ả': '\\u1EA2', 'Ã': '\\u00C3', 'Ạ': '\\u1EA0',\n",
    "        'Ă': '\\u0102', 'Ằ': '\\u1EB0', 'Ắ': '\\u1EAE', 'Ẳ': '\\u1EB2', 'Ẵ': '\\u1EB4', 'Ặ': '\\u1EB6',\n",
    "        'Â': '\\u00C2', 'Ầ': '\\u1EA6', 'Ấ': '\\u1EA4', 'Ẩ': '\\u1EA8', 'Ẫ': '\\u1EAA', 'Ậ': '\\u1EAC',\n",
    "        'È': '\\u00C8', 'É': '\\u00C9', 'Ẻ': '\\u1EBA', 'Ẽ': '\\u1EBC', 'Ẹ': '\\u1EB8',\n",
    "        'Ê': '\\u00CA', 'Ề': '\\u1EC0', 'Ế': '\\u1EBE', 'Ể': '\\u1EC2', 'Ễ': '\\u1EC4', 'Ệ': '\\u1EC6',\n",
    "        'Ì': '\\u00CC', 'Í': '\\u00CD', 'Ỉ': '\\u1EC8', 'Ĩ': '\\u0128', 'Ị': '\\u1ECA',\n",
    "        'Ò': '\\u00D2', 'Ó': '\\u00D3', 'Ỏ': '\\u1ECE', 'Õ': '\\u00D5', 'Ọ': '\\u1ECC',\n",
    "        'Ô': '\\u00D4', 'Ồ': '\\u1ED2', 'Ố': '\\u1ED0', 'Ổ': '\\u1ED4', 'Ỗ': '\\u1ED6', 'Ộ': '\\u1ED8',\n",
    "        'Ơ': '\\u01A0', 'Ờ': '\\u1EDC', 'Ớ': '\\u1EDA', 'Ở': '\\u1EDE', 'Ỡ': '\\u1EE0', 'Ợ': '\\u1EE2',\n",
    "        'Ù': '\\u00D9', 'Ú': '\\u00DA', 'Ủ': '\\u1EE6', 'Ũ': '\\u0168', 'Ụ': '\\u1EE4',\n",
    "        'Ư': '\\u01AF', 'Ừ': '\\u1EEA', 'Ứ': '\\u1EE8', 'Ử': '\\u1EEC', 'Ữ': '\\u1EEE', 'Ự': '\\u1EF0',\n",
    "        'Ỳ': '\\u1EF2', 'Ý': '\\u00DD', 'Ỷ': '\\u1EF6', 'Ỹ': '\\u1EF8', 'Ỵ': '\\u1EF4',\n",
    "        'Đ': '\\u0110'\n",
    "    }\n",
    "    # Chuyển đổi văn bản\n",
    "    normalized_text = \"\"\n",
    "    for char in text:\n",
    "        if char in vietnamese_chars:\n",
    "            normalized_text += vietnamese_chars[char]\n",
    "        else:\n",
    "            normalized_text += char\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    with open(\"vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = set(f.read().splitlines())\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "        return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text_lower = text.lower()\n",
    "    text_remove_special = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "    text_remove_space = text_remove_special.strip()\n",
    "    text_to_unicode = normalize_to_unicode(text_remove_space)\n",
    "    tokenized_text = ViTokenizer.tokenize(text_to_unicode)\n",
    "    text_remove_stopword = remove_stopwords(tokenized_text)\n",
    "    return text_remove_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathUrl = 'sort_label.csv'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = normalize_text(text)\n",
    "    return text\n",
    "df = pd.read_csv(pathUrl) \n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "print(processed_texts)\n",
    "processed_df = pd.DataFrame({'text': processed_texts, 'label': labels})\n",
    "processed_df.to_csv('clean_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(document):\n",
    "    \n",
    "    words = document.lower().split()\n",
    "    word_count = {}\n",
    "    total_words = len(words)\n",
    "    for word in words:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    tf = {word: count / total_words for word, count in word_count.items()}\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    \n",
    "    N = len(documents)\n",
    "    word_df = {}\n",
    "    for doc in documents:\n",
    "        words = set(doc.split())\n",
    "        for word in words:\n",
    "            word_df[word] = word_df.get(word, 0) + 1\n",
    "    idf = {word: math.log((N + 1) / (df + 1)) + 1 for word, df in word_df.items()}\n",
    "    return idf\n",
    "\n",
    "def compute_tf_idf(documents, idf_scores=None):\n",
    "    \n",
    "    if idf_scores is None:\n",
    "        idf_scores = compute_idf(documents)\n",
    "    tf_idf = []\n",
    "    for doc in documents:\n",
    "        tf_scores = compute_tf(doc)\n",
    "        tf_idf_s = {word: tf_scores[word] * idf_scores.get(word, 0) for word in tf_scores}\n",
    "        tf_idf.append(tf_idf_s)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_matrix(tf_idf_scores, vocabulary):\n",
    "    matrix = [[doc_scores.get(word, 0) for word in vocabulary] for doc_scores in tf_idf_scores]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for saving model and data\n",
    "MODEL_PATH = 'train/svm_model.pkl'\n",
    "VOCAB_PATH = 'train/vocabulary.pkl'\n",
    "IDF_PATH = 'train/idf_scores.pkl'\n",
    "\n",
    "def predict_with_model(X_test_list):\n",
    "    if not (os.path.exists(MODEL_PATH) and os.path.exists(VOCAB_PATH) and os.path.exists(IDF_PATH)):\n",
    "        print(\"No saved model or data found. Please train the model first!\")\n",
    "        return None\n",
    "\n",
    "    # Load model, vocabulary, and IDF scores\n",
    "    svm_model = joblib.load(MODEL_PATH)\n",
    "    vocabulary = joblib.load(VOCAB_PATH)\n",
    "    idf_scores = joblib.load(IDF_PATH)\n",
    "\n",
    "    # Compute TF-IDF for test data\n",
    "    tf_idf_test = compute_tf_idf(X_test_list, idf_scores)\n",
    "    X_test_matrix = convert_to_matrix(tf_idf_test, vocabulary)\n",
    "    X_test_array = np.array(X_test_matrix)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = svm_model.predict(X_test_array)\n",
    "    return y_pred\n",
    "\n",
    "def predict_text(text):\n",
    "    if not (os.path.exists(MODEL_PATH) and os.path.exists(VOCAB_PATH) and os.path.exists(IDF_PATH)):\n",
    "        print(\"No saved model or data found. Please train the model first!\")\n",
    "        return None\n",
    "    svm_model = joblib.load(MODEL_PATH)\n",
    "    vocabulary = joblib.load(VOCAB_PATH)\n",
    "    idf_scores = joblib.load(IDF_PATH)\n",
    "\n",
    "    text_list = [text]  \n",
    "    tf_idf = compute_tf_idf(text_list, idf_scores)\n",
    "    text_matrix = convert_to_matrix(tf_idf, vocabulary)\n",
    "    text_array = np.array(text_matrix)\n",
    "\n",
    "    # Predict class and probability\n",
    "    prediction = svm_model.predict(text_array)[0]\n",
    "    confidence = svm_model.predict_proba(text_array)[0][prediction]  # Lấy xác suất của class dự đoán\n",
    "\n",
    "    result = \"Lừa đảo\" if prediction == 1 else \"Hợp lệ\"\n",
    "    print(f\"Dự đoán: {result} (Độ tin cậy: {confidence:.2%})\")\n",
    "    return prediction, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_list, y_train):\n",
    "    idf_scores = compute_idf(X_train_list)\n",
    "    tf_idf_train = compute_tf_idf(X_train_list, idf_scores)\n",
    "    vocabulary = sorted(set().union(*[doc_scores.keys() for doc_scores in tf_idf_train]))\n",
    "\n",
    "    tf_idf_train = compute_tf_idf(X_train_list, idf_scores)\n",
    "    X_train_matrix = convert_to_matrix(tf_idf_train, vocabulary)\n",
    "    X_train_array = np.array(X_train_matrix)\n",
    "\n",
    "    # Train SVM model with optimized parameters\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf',     # Thay đổi kernel thành rbf\n",
    "        C=100,            # Giá trị C được điều chỉnh\n",
    "        gamma='scale',    # Điều chỉnh gamma\n",
    "        probability=True, \n",
    "        random_state=42\n",
    "    )\n",
    "    svm_model.fit(X_train_array, y_train)\n",
    "\n",
    "    joblib.dump(svm_model, MODEL_PATH)\n",
    "    joblib.dump(vocabulary, VOCAB_PATH)\n",
    "    joblib.dump(idf_scores, IDF_PATH)\n",
    "\n",
    "    print(\"Model trained and saved successfully!\")\n",
    "    return svm_model, vocabulary, idf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved successfully!\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       478\n",
      "           1       1.00      1.00      1.00       523\n",
      "\n",
      "    accuracy                           1.00      1001\n",
      "   macro avg       1.00      1.00      1.00      1001\n",
      "weighted avg       1.00      1.00      1.00      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "X = data['text'].fillna('')  \n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_list = X_train.tolist()\n",
    "X_test_list = X_test.tolist()\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    svm_model, vocabulary, idf_scores = train_model(X_train_list, y_train)\n",
    "else:\n",
    "    print(\"Model already exists, skipping training.\")\n",
    "\n",
    "y_pred = predict_with_model(X_test_list)\n",
    "\n",
    "if y_pred is not None:\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  mua hàng giá rẻ\n",
      "Dự đoán: Hợp lệ (Độ tin cậy: 73.03%)\n",
      "\n",
      "\n",
      "Text:  bán khóa học giá rẻ\n",
      "Dự đoán: Lừa đảo (Độ tin cậy: 93.30%)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   example = \"mua hàng giá rẻ\"\n",
    "   example_2 = \"bán khóa học giá rẻ\"\n",
    "   print(\"Text: \",example)\n",
    "   predict_text(example)\n",
    "   print(\"\\n\")\n",
    "   print(\"Text: \",example_2)\n",
    "   predict_text(example_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
